{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mutual_information.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMJbYT9K+Q0bBeTZYvIxd9d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.5 64-bit ('functorch': conda)"
    },
    "interpreter": {
      "hash": "fe44645ae337dd8afaf2eb291ed01fb9c025d8580e64855ce16722fc232964f6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VRehnberg/mutual-information/blob/main/mutual_information.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABjQs-_zOBIi",
        "outputId": "9b039e8e-b9c6-45e5-da24-612543edba5f"
      },
      "source": [
        "import sys\n",
        "sys.path.append(\"torch-utils/src\")\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGuScJ-yt2z9"
      },
      "source": [
        "import torch\n",
        "from torch import nn, linalg\n",
        "from torch.autograd.functional import jacobian\n",
        "\n",
        "import functorch\n",
        "\n",
        "from torchutils import batched_jacobian\n",
        "from torchutils.kmeans import kmeans\n",
        "from torchutils.plotting import tensorshow\n",
        "from torchutils.named_tensors import vmap\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6Wphq9FsMwx"
      },
      "source": [
        "# Mutual Information\n",
        "This notebook was written to investigate a few different ways to estimate the mutual information between random variables from sampled data. This is then compared with the true mutual information.\n",
        "\n",
        "1. Analytical mutual information TODO\n",
        "2. Jacobian/Hessian based mutual information TODO\n",
        "3. Quantized/binned mutual information TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McncszxCma_A"
      },
      "source": [
        "## Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jfqX6vS8MD_"
      },
      "source": [
        "class NormalLinear(nn.Linear):\n",
        "    def __init__(self, input_shape, output_shape):\n",
        "        super().__init__(input_shape, output_shape)\n",
        "        self.input_shape = input_shape\n",
        "    \n",
        "    def output_mutual_information(self, partition):\n",
        "        '''\n",
        "            partition (BoolTensor): n_modules Ã— output_shape\n",
        "        '''\n",
        "        if (partition.int().sum(0) > 1).any():\n",
        "            raise NotImplementedError(\"MI for overlapping modules not implemented.\")\n",
        "\n",
        "        weight, bias = self.parameters()\n",
        "        #mean = bias\n",
        "        cov_full = weight @ weight.T\n",
        "\n",
        "        # Check ranks (this is unescessary if full rank)\n",
        "        rank0 = linalg.matrix_rank(cov_full)\n",
        "        rank1 = torch.sum(torch.hstack([\n",
        "            linalg.matrix_rank(cov_full[mask, :][:,mask]) for mask in partition\n",
        "        ]))\n",
        "        if rank0 < rank1:\n",
        "            return float(\"inf\")\n",
        "        elif rank1 != cov_full.size(0):\n",
        "            raise NotImplementedError()\n",
        "        \n",
        "        # Compute MI\n",
        "        det0 = cov_full.det()\n",
        "        det1 = torch.prod(torch.hstack([\n",
        "            cov_full[mask, :][:,mask].det() for mask in partition\n",
        "        ]))\n",
        "        return -0.5 * torch.log(det0 / det1)\n",
        "        \n",
        "    def sample(self, batch_shape):\n",
        "        return torch.randn(batch_shape, self.input_shape, requires_grad=True)\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAVLB0eUCEb6",
        "outputId": "cce97ee8-a87a-45ef-c162-1db5365734dc"
      },
      "source": [
        "n_modules = 2\n",
        "in_size, out_size = (15, 7)\n",
        "nl = NormalLinear(in_size, out_size)\n",
        "partition = nn.functional.one_hot(torch.randint(n_modules, (out_size,))).bool().T\n",
        "with torch.no_grad():\n",
        "    print(nl.output_mutual_information(partition))\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.6027)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzLolCqexxsU"
      },
      "source": [
        "## Mutual information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbDQJO7HoDCd"
      },
      "source": [
        "def jacobian_mutual_information(jac_full, jac_blocks):\n",
        "    assert jac_full.ndim == 3\n",
        "\n",
        "    # Covariances\n",
        "    def det(jac):\n",
        "        return jac.bmm(jac.transpose(1, 2)).det()\n",
        "\n",
        "    det_full = det(jac_full)\n",
        "    det_blocks = torch.hstack([det(jac_block).view(-1, 1) for jac_block in jac_blocks])\n",
        "\n",
        "    # Local mutual information\n",
        "    jmi = -0.5 * torch.log(det_full / torch.prod(det_blocks, 1))\n",
        "\n",
        "    return jmi.mean(0)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyjcdxJWA6w2",
        "outputId": "54478b3c-83d6-4947-ef61-c0087b3eab88"
      },
      "source": [
        "jac_full = torch.rand(7, 10, 30)\n",
        "partition = torch.randint(3, (10,))\n",
        "jac_blocks = [jac_full[:, id==partition, :] for id in torch.unique(partition)]\n",
        "jacobian_mutual_information(jac_full, jac_blocks)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.3925)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kH0T7-pdyW5y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "649d6870-da71-4a78-f65d-31514c843e77"
      },
      "source": [
        "def quantized_mutual_information(\n",
        "    activations,\n",
        "    partition,\n",
        "    n_bins,\n",
        "    cluster_method=\"kmeans\",\n",
        "    return_full=False,\n",
        "):\n",
        "    if partition.dtype != torch.bool:\n",
        "        raise TypeError(\"Datatype of partition should be bool.\")\n",
        "    device = activations.device\n",
        "    n_samples = activations.size(\"sample\")\n",
        "    n_neurons = activations.size(\"neuron\")\n",
        "    n_modules = partition.size(\"module\")\n",
        "    assert n_neurons == partition.size(\"neuron\")\n",
        "    assert n_samples >= n_bins\n",
        "\n",
        "    if cluster_method==\"kmeans\":\n",
        "        def quantize(points):\n",
        "            # Cosine similarity\n",
        "            points = (points - points.mean(0, keepdim=True)) / points.std(0, keepdim=True)\n",
        "            return kmeans(points, n_bins)\n",
        "    else:\n",
        "        raise ValueError()\n",
        "\n",
        "    quantized_activations = vmap(lambda mask: quantize(activations[:, mask]), (\"neuron\",))(\n",
        "        partition\n",
        "    )\n",
        "    #print(partition.size(), partition.names)\n",
        "    #print(activations.size(), activations.names)\n",
        "    #functorch.vmap(lambda mask: quantize(activations[:, mask]), (0,), (1,))(\n",
        "    #    partition\n",
        "    #)\n",
        "    #quantized_activations = torch.hstack([\n",
        "    #    quantize(activations.masked_select(mask)) for mask in partition.unbind(\"neuron\")\n",
        "    #])\n",
        "    \n",
        "    # Compute pmfs\n",
        "    activations_onehot = nn.functional.one_hot(quantized_activations).refine_names(\"bin\").float()\n",
        "    p_xy = activations_onehot @ activations_onehot.rename(neuron=\"neuron2\", bin=\"bin2\") / batch_size\n",
        "    #torch.einsum(\"bij, bkl -> ikjl\", activations_onehot, activations_onehot) / batch_size\n",
        "\n",
        "    print(p_xy.names)\n",
        "    #tensorshow(p_xy, )\n",
        "\n",
        "    # Compute pairwise mutual information\n",
        "    p_x = p_xy.diagonal(dim1=\"neuron\", dim2=\"neuron2\").diagonal(dim1=\"bin\", dim2=\"bin2\").refine_names(\"neuron\", \"bin\")\n",
        "    p_y = p_x.rename(bin=\"bin2\", neuron=\"neuron2\")\n",
        "    #p_x = torch.einsum(\"iikk -> ik\", p_xy)\n",
        "    qmin = p_xy.div(p_x).div(p_y).pow(p_xy).log().sum((\"neuron\", \"neuron2\"))\n",
        "    assert qmin.size(\"bin\") == qmin.size(\"bin2\") and qmin.ndim == 2\n",
        "    return qmin\n",
        "    \n",
        "\n",
        "n_modules = 2\n",
        "in_size, out_size = (15, 7)\n",
        "activations = torch.rand(2000, out_size).refine_names(\"sample\", \"neuron\")\n",
        "partition = nn.functional.one_hot(\n",
        "    torch.randint(n_modules, (out_size,))\n",
        ").refine_names(\"neuron\", \"module\").bool()\n",
        "with torch.no_grad():\n",
        "    print(quantized_mutual_information(activations, partition, 10))\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/home/viktor/miniconda3/envs/functorch/lib/python3.9/site-packages/torch/_tensor.py:768: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1304.)\n  return super(Tensor, self).refine_names(names)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "batched == nullptrINTERNAL ASSERT FAILED at \"/tmp/pip-req-build-ym9h0m9k/functorch/csrc/DynamicLayer.cpp\":250, please report a bug to PyTorch. ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-007b41e317f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     59\u001b[0m ).refine_names(\"neuron\", \"module\").bool()\n\u001b[1;32m     60\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquantized_mutual_information\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-007b41e317f3>\u001b[0m in \u001b[0;36mquantized_mutual_information\u001b[0;34m(activations, partition, n_bins, cluster_method, return_full)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     quantized_activations = vmap(lambda mask: quantize(activations[:, mask]), (\"neuron\",))(\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mpartition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     )\n",
            "\u001b[0;32m~/Documents/Alignment/aisc5/modularity/mutual-information/torch-utils/src/torchutils/named_tensors.py\u001b[0m in \u001b[0;36mvfunc\u001b[0;34m(inputs)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0min_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mover_dims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mdims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mover_dims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunctorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefine_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mover_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/functorch/lib/python3.9/site-packages/functorch/_src/vmap.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0mbatched_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_batched_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_in_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmap_level\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m             \u001b[0mbatched_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatched_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_unwrap_batched\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatched_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmap_level\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/Documents/Alignment/aisc5/modularity/mutual-information/torch-utils/src/torchutils/named_tensors.py\u001b[0m in \u001b[0;36mnfunc\u001b[0;34m(input)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mnonlocal\u001b[0m \u001b[0mout_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0min_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mout_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-007b41e317f3>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(mask)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     quantized_activations = vmap(lambda mask: quantize(activations[:, mask]), (\"neuron\",))(\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mpartition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     )\n",
            "\u001b[0;31mRuntimeError\u001b[0m: batched == nullptrINTERNAL ASSERT FAILED at \"/tmp/pip-req-build-ym9h0m9k/functorch/csrc/DynamicLayer.cpp\":250, please report a bug to PyTorch. "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_modules = 2\n",
        "in_size, out_size = (15, 2)\n",
        "activations = torch.rand(2000, out_size)\n",
        "partition = torch.eye(2, dtype=bool)\n",
        "with torch.no_grad():\n",
        "    print(quantized_mutual_information(activations, partition, 10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Gaussian test set-up\n",
        "n_modules = 2\n",
        "in_size, out_size = (15, 7)\n",
        "batch_size = 2000\n",
        "network = NormalLinear(in_size, out_size)\n",
        "x = network.sample(batch_size)\n",
        "activations = network(x)\n",
        "\n",
        "partition = nn.functional.one_hot(torch.randint(n_modules, (out_size,))).bool().T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# True mutual information\n",
        "mi = network.output_mutual_information(partition)\n",
        "print(f\"MI: {mi}\")\n",
        "\n",
        "# Local mutual information through Jacobian\n",
        "jac_full = batched_jacobian(network, x)\n",
        "jac_blocks = [jac_full[:, mask, :] for mask in partition]\n",
        "lmi = jacobian_mutual_information(jac_full, jac_blocks)\n",
        "print(f\"LMI: {lmi}\")\n",
        "\n",
        "# Quantized mutual information through clustering\n",
        "for k in range(2, 11, 2):\n",
        "    with torch.no_grad():\n",
        "        qmi = quantized_mutual_information(activations, partition, k)\n",
        "    print(f\"QMI k={k}: {qmi}\")\n"
      ]
    }
  ]
}