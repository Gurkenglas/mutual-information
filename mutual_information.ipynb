{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mutual_information.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMJbYT9K+Q0bBeTZYvIxd9d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.10 64-bit ('ml': conda)"
    },
    "interpreter": {
      "hash": "a41517927f8b2b7c955ca7cdf066b814679ae4d55f949738367ae73352dc8f7c"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VRehnberg/mutual-information/blob/main/mutual_information.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABjQs-_zOBIi",
        "outputId": "9b039e8e-b9c6-45e5-da24-612543edba5f"
      },
      "source": [
        "running_on_colab = \"google.colab\" in str(get_ipython())\n",
        "if running_on_colab:\n",
        "    %pip install git+https://github.com/VRehnberg/torch-utils.git\n",
        "else:\n",
        "    !git clone https://github.com/VRehnberg/torch-utils && cp -alf torch-utils/src/torchutils . && rm -rf torch-utils\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'torch-utils'...\n",
            "remote: Enumerating objects: 111, done.\u001b[K\n",
            "remote: Counting objects: 100% (111/111), done.\u001b[K\n",
            "remote: Compressing objects: 100% (67/67), done.\u001b[K\n",
            "remote: Total 111 (delta 37), reused 90 (delta 29), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (111/111), 20.25 KiB | 2.89 MiB/s, done.\n",
            "Resolving deltas: 100% (37/37), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGuScJ-yt2z9"
      },
      "source": [
        "import torch\n",
        "from torch import nn, linalg\n",
        "from torch.autograd.functional import jacobian\n",
        "\n",
        "from torchutils import batched_jacobian\n",
        "from torchutils.kmeans import kmeans"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6Wphq9FsMwx"
      },
      "source": [
        "# Mutual Information\n",
        "This notebook was written to investigate a few different ways to estimate the mutual information between random variables from sampled data. This is then compared with the true mutual information.\n",
        "\n",
        "1. Analytical mutual information TODO\n",
        "2. Jacobian/Hessian based mutual information TODO\n",
        "3. Quantized/binned mutual information TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McncszxCma_A"
      },
      "source": [
        "## Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jfqX6vS8MD_"
      },
      "source": [
        "class NormalLinear(nn.Linear):\n",
        "    def __init__(self, input_shape, output_shape):\n",
        "        super().__init__(input_shape, output_shape)\n",
        "        self.input_shape = input_shape\n",
        "    \n",
        "    def output_mutual_information(self, partition):\n",
        "        '''\n",
        "            partition (BoolTensor): n_modules Ã— output_shape\n",
        "        '''\n",
        "        if (partition.int().sum(0) > 1).any():\n",
        "            raise NotImplementedError(\"MI for overlapping modules not implemented.\")\n",
        "\n",
        "        weight, bias = self.parameters()\n",
        "        #mean = bias\n",
        "        cov_full = weight @ weight.T\n",
        "\n",
        "        # Check ranks (this is unescessary if full rank)\n",
        "        rank0 = linalg.matrix_rank(cov_full)\n",
        "        rank1 = torch.sum(torch.hstack([\n",
        "            linalg.matrix_rank(cov_full[mask, :][:,mask]) for mask in partition\n",
        "        ]))\n",
        "        if rank0 < rank1:\n",
        "            return float(\"inf\")\n",
        "        elif rank1 != cov_full.size(0):\n",
        "            raise NotImplementedError()\n",
        "        \n",
        "        # Compute MI\n",
        "        det0 = cov_full.det()\n",
        "        det1 = torch.prod(torch.hstack([\n",
        "            cov_full[mask, :][:,mask].det() for mask in partition\n",
        "        ]))\n",
        "        return -0.5 * torch.log(det0 / det1)\n",
        "        \n",
        "    def sample(self, batch_shape):\n",
        "        return torch.randn(batch_shape, self.input_shape, requires_grad=True)\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAVLB0eUCEb6",
        "outputId": "cce97ee8-a87a-45ef-c162-1db5365734dc"
      },
      "source": [
        "n_modules = 2\n",
        "in_size, out_size = (15, 7)\n",
        "nl = NormalLinear(in_size, out_size)\n",
        "partition = nn.functional.one_hot(torch.randint(n_modules, (out_size,))).bool().T\n",
        "with torch.no_grad():\n",
        "    print(nl.output_mutual_information(partition))\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.5537)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzLolCqexxsU"
      },
      "source": [
        "## Mutual information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbDQJO7HoDCd"
      },
      "source": [
        "def jacobian_mutual_information(jac_full, jac_blocks):\n",
        "    assert jac_full.ndim == 3\n",
        "\n",
        "    # Covariences\n",
        "    def det(jac):\n",
        "        return jac.bmm(jac.transpose(1, 2)).det()\n",
        "\n",
        "    det_full = det(jac_full)\n",
        "    det_blocks = torch.hstack([det(jac_block).view(-1, 1) for jac_block in jac_blocks])\n",
        "\n",
        "    # Local mutual information\n",
        "    jmi = -0.5 * torch.log(det_full / torch.prod(det_blocks, 1))\n",
        "\n",
        "    return jmi.mean(0)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyjcdxJWA6w2",
        "outputId": "54478b3c-83d6-4947-ef61-c0087b3eab88"
      },
      "source": [
        "jac_full = torch.rand(7, 10, 30)\n",
        "partition = torch.randint(3, (10,))\n",
        "jac_blocks = [jac_full[:, id==partition, :] for id in torch.unique(partition)]\n",
        "jacobian_mutual_information(jac_full, jac_blocks)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.2743)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PC-Yoc1ueRcJ",
        "outputId": "d5169f2f-78aa-465b-afda-c12dac95b78a"
      },
      "source": [
        "help(kmeans)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on function kmeans in module torchutils.kmeans:\n\nkmeans(points, k, global_start=1, max_iter=None, reinitialize_empty_clusters=True, verbose=False)\n    Naive K-means with parallelized global start.\n    \n    Arguments:\n        points (Tensor): Number of points times number of features.\n        k (int): Number of clusters.\n        global_start (int): Different intializations that are run in parallel.\n            Default 1.\n        reinatialize_empty_clusters (bool): If empty clusters are reinitialized.\n            Default True.\n        verbose (bool): Controls verbosity. Default True. \n    \n    Returns:\n        i_cluster (LongTensor): What cluster each point belongs to.\n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kH0T7-pdyW5y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "649d6870-da71-4a78-f65d-31514c843e77"
      },
      "source": [
        "def quantized_mutual_information(activations, partition, n_bins, cluster_method=\"kmeans\"):\n",
        "    if not isinstance(partition, torch.BoolTensor):\n",
        "        raise TypeError(\"Datatype of partition should be BoolTensor.\")\n",
        "    device = activations.device\n",
        "    batch_size, n_features = activations.shape\n",
        "    n_parts = partition.size(0)\n",
        "    assert n_features == partition.size(1)\n",
        "    assert batch_size >= n_bins\n",
        "\n",
        "    if cluster_method==\"kmeans\":\n",
        "        def quantize(points):\n",
        "            # Cosine similarity\n",
        "            points = (points - points.mean(0, keepdim=True)) / points.std(0, keepdim=True)\n",
        "            return kmeans(points, n_bins)\n",
        "    else:\n",
        "        raise ValueError()\n",
        "\n",
        "    quantized_activations = torch.zeros((batch_size, n_parts), dtype=int, device=device)\n",
        "    for i_part, mask in enumerate(partition):\n",
        "        quantized_activations[:, i_part] = quantize(activations[:, mask])\n",
        "\n",
        "    # Compute pmfs\n",
        "    activations_onehot = nn.functional.one_hot(quantized_activations).float()\n",
        "    p_xy = torch.einsum(\"bij, bkl -> ikjl\", activations_onehot, activations_onehot) / batch_size\n",
        "\n",
        "    print(activations_onehot.shape)\n",
        "    print(p_xy.shape)\n",
        "\n",
        "    # Compute pairwise mutual information\n",
        "    p_x = torch.einsum(\"iikk -> ik\", p_xy)\n",
        "    qmin = p_xy.div(p_x.unsqueeze(0).unsqueeze(2)).div(p_x.unsqueeze(1).unsqueeze(3)).pow(p_xy).log().sum((2, 3))\n",
        "    return qmin\n",
        "    \n",
        "\n",
        "n_modules = 2\n",
        "in_size, out_size = (15, 7)\n",
        "activations = torch.rand(2000, out_size)\n",
        "partition = nn.functional.one_hot(torch.randint(n_modules, (out_size,))).bool().T\n",
        "with torch.no_grad():\n",
        "    print(quantized_mutual_information(activations, partition, 10))\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2000, 2, 10])\ntorch.Size([2, 2, 10, 10])\ntensor([[2.2919, 0.0236],\n        [0.0236, 2.2965]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Gaussian test set-up\n",
        "n_modules = 2\n",
        "in_size, out_size = (15, 7)\n",
        "batch_size = 20\n",
        "network = NormalLinear(in_size, out_size)\n",
        "x = network.sample(batch_size)\n",
        "activations = network(x)\n",
        "\n",
        "partition = nn.functional.one_hot(torch.randint(n_modules, (out_size,))).bool().T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MI: 0.7629395723342896\nLMI: 0.7629393935203552\nQMI: tensor([[2.9957, 2.9957],\n        [2.9957, 2.9957]])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# True mutual information\n",
        "mi = network.output_mutual_information(partition)\n",
        "print(f\"MI: {mi}\")\n",
        "\n",
        "# Local mutual information through Jacobian\n",
        "jac_full = batched_jacobian(network, x)\n",
        "jac_blocks = [jac_full[:, mask, :] for mask in partition]\n",
        "lmi = jacobian_mutual_information(jac_full, jac_blocks)\n",
        "print(f\"LMI: {lmi}\")\n",
        "\n",
        "# Quantized mutual information through clustering\n",
        "with torch.no_grad():\n",
        "    qmi = quantized_mutual_information(activations, partition, 10)\n",
        "print(f\"QMI: {qmi}\") #TODO\n"
      ]
    }
  ]
}